
\documentclass{article}

\usepackage{hyperref}

\begin{document}

\section{Artifact Identification}

The main contribution of the paper is a new workflow that translates Fortran applications
into a data-centric representation called SDFG, applies data-flow optimizations in DaCe, and compiles the
applications to CPU and GPU.
We demonstrate that the semi-automatic optimization framework can parallelize applications
and generate optimized representation for CPUs and GPUs.
In the paper, we evaluate the new approach with microbenchmark and CloudSC, a large Fortran kernel
implementing cloud microphysics scheme.

We execute all benchmarks on cluster nodes, each one equipped with a 32-core AMD EPYC 7501 CPU @ 2.0 GHz,
512 GB memory, and NVIDIA Tesla V100 GPU with 32 GB of memory.
We use Python 3.10 with a development branch of DaCe, available
\href{https://github.com/spcl/dace/tree/fortran_frontend_candidate_2}{in the official GitHub repository}.
To compile generated code and baseline benchmarks, we use GNU GCC 10.2 and CUDA 11.8.

The computational artifact is \href{https://github.com/spcl/f2dace-artifact}{available on GitHub}
and consists of four main components:
\begin{enumerate}
  \item a DaCe translation frontend for Fortran applications,
  \item a set of new SDFG transformations,
  \item benchmark reproduction with configuration and execution scripts,
  \item and generated data used in the paper.
\end{enumerate}
With the artifact, one can generate all performance results presented in the paper: the evaluation
of CloudSC (Figures 1 and 10), as well as the microbenchmark evaluation (Figure 11).

\section{Reproducibility of Experiments}

The artifact repository consists of three main components: \texttt{data} contains results generated and presented
in the paper, \texttt{analysis} includes all scripts used to produce plots included in the paper,
and \texttt{microbenchmarks} and \texttt{cloudsc} contain software needed to reproduce the results obtained in the paper.
For each benchmark, we provide Python and Bash scripts automating the entire procedure:

The essential results of the paper include the performance of the new workflow with the CloudSC benchmark,
and a Fortran Pi microbenchmark.
Given the cluster nodes described in previous part, it should not take more than 5 hours to repeat
experiments in all configurations.

\subsection{CloudSC}

The benchmark is executed on CPU and GPU with the following configuration:
\begin{enumerate}
\item Repetitions = 20
\item CPU Threads = 1, 2, 4, 8, 16, 32
\item Size values = 4096, 8192, 16384, 32768, 65536, 131072, 262144
\item NPROMA values = 16 64 128
\end{enumerate}

To prepare microbenchmarks, run first \texttt{install\_deps.sh} in the \texttt{microbenchmarks} directory.
The script will install DaCe development branch at the commit version used for the experments.

\textbf{Baseline}.

\begin{enumerate}
\item First, build the baseline benchmarks by stepping into \texttt{cloudsc/original} directory and running
\texttt{install\_cloudsc.sh}.
\item SLURM scripts \texttt{job\_cpu\_c.sh}, \texttt{job\_cpu\_fortran.sh}, \texttt{job\_cuda.sh} and \texttt{job\_openacc.sh}
by replacing directory paths to the current location of the artifact repository and adjusting \texttt{sbatch}
configuration to your system (partition, nodelist).
\item the baseline benchmark repetitions for CPU, CUDA, and OpenACC by running
\texttt{run\_cpu.sh}, \texttt{run\_cuda.sh}, and \texttt{run\_openacc.sh}, respectively.
\item generated data can be found in \texttt{data/cloudsc/original/}.
\end{enumerate}

\textbf{DaCe, CPU and GPU (our results)}

\begin{enumerate}
\item First, enter the directories \texttt{cloudsc/dace/{cpu,gpu}\_ready\_sdfg}.
\item Build the benchmark by running the following scripts: \texttt{install\_deps.sh}, \texttt{install\_cloudsc.sh},
and \texttt{compile\_sdfg.sh}.
\item Adjust the SLURM scripts \texttt{job\_dace\_cpu.sh} and \texttt{job\_dace\_gpu\_a100.sh}
by replacing directory paths to the current location of the artifact repository and adjusting \texttt{sbatch}
configuration to your system (partition, nodelist).
\item Submit the baseline benchmark repetitions by running \texttt{submit.sh}.
\item The generated data can be found in \texttt{data/cloudsc/dace/{cpu,gpu}}.
\end{enumerate}

\textbf{Plotting}
%
To generate Figures 1 and 10, step into the directory \texttt{analysis/cloudsc}, run \texttt{process\_data.py},
and then execute scripts \texttt{plot\_fig1.py} and \texttt{plot\_fig10.py}. These generate plots in PDF that were
later improved aesthetically and assembled into the final version in the paper.

\subsection{Microbenchmarks}

To prepare microbenchmarks, run first \texttt{install\_deps.sh} in the \texttt{microbenchmarks} directory.
The script will install DaCe development branch at the commit version used for the experments.

The microbenchmark includes the parallel Pi computation. The benchmark is executed with the following
configuration:
\begin{enumerate}
\item Repetitions = 20
\item Size = 1000000000, 1250000000, 1500000000, 1750000000, 2000000000
\item CPU Threads = 1, 2, 4, 8, 16, 32
\end{enumerate}

\textbf{Baseline}

\begin{enumerate}
\item First, build the baseline PI benchmark by stepping into \texttt{microbenchmarks/pi/baseline} directory and running
\texttt{install\_deps.sh}.
\item Adjust the SLURM script \texttt{job\_pi.sh}
by replacing directory paths to the current location of the artifact repository and adjusting \texttt{sbatch}
configuration to your system (partition, nodelist).
\item Submit the baseline benchmark repetitions by running \texttt{submit.sh}.
\item The generated data can be found in \texttt{data/microbenchmarks/pi/baseline}.
\end{enumerate}

\textbf{DaCe, CPU and GPU (our results)}

\begin{enumerate}
\item First, build the optimized benchmark by stepping into \texttt{microbenchmarks/pi/dace/{cpu, gpu}}
and run \texttt{compile\_sdfg.sh}.
\item Adjust the SLURM script \texttt{job\_pi.sh}
by replacing directory paths to the current location of the artifact repository and adjusting \texttt{sbatch}
configuration to your system (partition, nodelist).
\item Submit the baseline benchmark repetitions by running \texttt{submit.sh}.
\item The generated data can be found in \texttt{data/microbenchmarks/pi/baseline}.
\end{enumerate}

\textbf{Plotting}
%
To generate Figure 11, step into the directory \texttt{analysis/mirobenchmarks} and run \texttt{process\_data.py},
followed by \texttt{plot\_pi.py}.

\end{document}

